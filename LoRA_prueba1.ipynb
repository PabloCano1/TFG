{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbd5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9e26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3eb2cc34c84ff4ad64a2015587d90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e81bc34fe4d486cac313dc91ef82971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281f660831d445fbae231d5f58862878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cdcdf8cff8469d90977c8c644b8faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5c6af761ba40919c4777c725c87b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b1980cd86f410aab4a51882b71fb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# Put your HF Token here\n",
    "os.environ['HF_TOKEN']=\"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\" # the token should have write access\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "dataset_name = \"arthurmello/paul-graham-essays\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    "def preprocess(sample):\n",
    "    prompt = sample[\"prompt\"]\n",
    "    response = sample[\"response\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    # Truncar la secuencia si es demasiado larga\n",
    "    tokens = tokenizer(text, truncation=True, max_length=2048)\n",
    "    return {\"text\": tokenizer.decode(tokens[\"input_ids\"])}\n",
    "    # return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(dataset_name,token=\"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\")\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"prompt\", \"response\"])\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f252dab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8e7b3b8a0242a0bf3ffabb5c2bc447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d2c85370bd4a5888c1b74f71419080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b792358194d941759e48d780f14d157d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             attn_implementation='eager',\n",
    "                                             device_map=\"cuda\",dtype=torch.bfloat16)\n",
    "model.config.use_cache = False\n",
    "# model.to(torch.bfloat16)\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880ca4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_dimension = 4\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.1\n",
    "\n",
    "peft_config = LoraConfig(r=rank_dimension,\n",
    "                         lora_alpha=lora_alpha,\n",
    "                         lora_dropout=lora_dropout,\n",
    "                         target_modules=[\n",
    "                             \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "                             \"o_proj\", \"gate_proj\", \"up_proj\",\n",
    "                             \"down_proj\"\n",
    "                             ],\n",
    "                         task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0863192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"PabloCano1\" # replace with your Hugging Face username\n",
    "output_dir = \"gemma-3-1b-it-paul-graham\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "num_train_epochs=10\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 500\n",
    "# max_grad_norm= 1 ##########\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    # logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    # max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=True,\n",
    "    hub_private_repo=False,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    packing=False,\n",
    "    max_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb39e7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62775df1f9ab4127a4b0f5143d9dfcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37164c50c214180befd891830244931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5319 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee4d27aea1d45558ae47ffd6c4543de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08622fb3cf8c4e17b7ccc56d842904f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dbfb7e5c22431193a7e3ae576bba97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614c105a5b784e01ae9f24ffe6926f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 17:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>3.310050</td>\n",
       "      <td>3.312529</td>\n",
       "      <td>100096.000000</td>\n",
       "      <td>0.375534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.183500</td>\n",
       "      <td>3.116985</td>\n",
       "      <td>3.163916</td>\n",
       "      <td>200192.000000</td>\n",
       "      <td>0.399498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.993900</td>\n",
       "      <td>2.988381</td>\n",
       "      <td>3.038912</td>\n",
       "      <td>300288.000000</td>\n",
       "      <td>0.417189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.939900</td>\n",
       "      <td>2.955033</td>\n",
       "      <td>3.008089</td>\n",
       "      <td>400384.000000</td>\n",
       "      <td>0.424626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.879000</td>\n",
       "      <td>2.941576</td>\n",
       "      <td>2.987388</td>\n",
       "      <td>500480.000000</td>\n",
       "      <td>0.427114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.854000</td>\n",
       "      <td>2.933782</td>\n",
       "      <td>2.972882</td>\n",
       "      <td>600576.000000</td>\n",
       "      <td>0.427137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.886800</td>\n",
       "      <td>2.929042</td>\n",
       "      <td>2.966380</td>\n",
       "      <td>700672.000000</td>\n",
       "      <td>0.427461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.903700</td>\n",
       "      <td>2.927042</td>\n",
       "      <td>2.967089</td>\n",
       "      <td>800768.000000</td>\n",
       "      <td>0.426557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.830300</td>\n",
       "      <td>2.924321</td>\n",
       "      <td>2.964019</td>\n",
       "      <td>900864.000000</td>\n",
       "      <td>0.428924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.830500</td>\n",
       "      <td>2.926177</td>\n",
       "      <td>2.964804</td>\n",
       "      <td>1000960.000000</td>\n",
       "      <td>0.428310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=520, training_loss=2.9647464752197266, metrics={'train_runtime': 1035.012, 'train_samples_per_second': 1.981, 'train_steps_per_second': 0.502, 'total_flos': 511988253327360.0, 'train_loss': 2.9647464752197266, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbe9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f92321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample: Write an essay discussing the nature of online trolling, its historical origins, causes, and how it affects internet communities. Include strategies to mitigate trolling and explore whether technical solutions or community culture changes are more effective in maintaining constructive online environments. Consider how a community’s tolerance for trolling influences its overall atmosphere and user engagement.<eos> \n",
      "\n",
      "\n",
      "\n",
      "Write an essay discussing the nature of online trolling, its historical origins, causes, and how it affects internet communities. Include strategies to mitigate trolling and explore whether technical solutions or community culture changes are more effective in maintaining constructive online environments. Consider how a community’s tolerance for trolling influences its overall atmosphere and user engagement.<eos>\n",
      "\n",
      "The term trolling is used to describe the practice of trolling online. It is a term that describes the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice of trolling online. It is used to describe the practice\n"
     ]
    }
   ],
   "source": [
    "# Obtener el primer ejemplo del conjunto de test\n",
    "test_sample = \"Write an essay discussing the nature of online trolling, its historical origins, causes, and how it affects internet communities. Include strategies to mitigate trolling and explore whether technical solutions or community culture changes are more effective in maintaining constructive online environments. Consider how a community’s tolerance for trolling influences its overall atmosphere and user engagement.<eos>\"\n",
    "print(\"Test sample:\", test_sample, \"\\n\\n\\n\")\n",
    "# Tokenizar la entrada\n",
    "inputs = tokenizer(test_sample, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(model.device)\n",
    "\n",
    "# Generar predicción\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599e455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
