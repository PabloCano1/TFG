{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdc78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a896991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "# Corrección: cargar JSON local correctamente\n",
    "dataset = load_dataset('json', data_files='./data/discourse_qa.json')\n",
    "#remove columns that are not needed \"question\" and \"answer\"\n",
    "dataset = dataset.remove_columns([\"question\", \"answer\"])\n",
    "#split train and test\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe09068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "perplexity = evaluate.load(\"perplexity\")\n",
    "\n",
    "# ejemplos planos\n",
    "# predictions = [\"respuesta generada por el modelo 1\", \"respuesta generada 2\"]\n",
    "# reales = [\"respuesta real 1\", \"respuesta real 2\"]\n",
    "\n",
    "reales = [i['text'] for i in dataset['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4989cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08294344",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"PabloCano1/llama1b-entreno\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             attn_implementation='eager',\n",
    "                                             device_map=\"cuda\",dtype=torch.bfloat16)\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c24ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 884/884 [10:20<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "reales_only_response = []\n",
    "for real in tqdm(reales):\n",
    "    real_no_response = real[:real.find('### Expected Response:') + len(\"### Expected Response:\")].strip()\n",
    "    inputs = tokenizer(real_no_response, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=None,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.90,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(f\"{output=}\")\n",
    "    # print(f\"{real=}\")\n",
    "    reales_only_response.append(real[real.find('### Expected Response:') + len('### Expected Response:'):].strip())\n",
    "    predictions.append(output)\n",
    "    # predictions.append(output[output.find('### Expected Response:')+len('### Expected Response:'):].strip())\n",
    "    # print(f\"{reales_only_response[-1]=}\")\n",
    "    # print(f\"{predictions[-1]=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c95581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.0551982777072815\n",
      "ROUGE: {'rouge1': np.float64(0.14818577530685856), 'rouge2': np.float64(0.02640262530774247), 'rougeL': np.float64(0.09756200040479551), 'rougeLsum': np.float64(0.14008163406230378)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean perplexity: 2.845599216118297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Para BLEU la librería espera referencias como lista de listas (cada pred puede tener varias refs)\n",
    "references_for_bleu = [[r] for r in reales_only_response]   # [[\"respuesta real 1\"], [\"respuesta real 2\"]]\n",
    "\n",
    "# Opción 1: pasar cadenas y dejar que evaluate haga el tokenizado internamente\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references_for_bleu)\n",
    "print(\"BLEU:\", bleu_result[\"bleu\"])\n",
    "\n",
    "# ROUGE normalmente recibe listas de strings\n",
    "rouge_result = rouge.compute(predictions=predictions, references=reales_only_response)\n",
    "print(\"ROUGE:\", rouge_result)\n",
    "\n",
    "# Calcular perplexity usando el módulo ya cargado\n",
    "perplexities = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(predictions, desc=\"Calculando perplexities\",leave=False):\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        # Para modelos causales: pasar labels iguales a input_ids calcula la loss por token\n",
    "        outputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "        loss = outputs.loss.item()  # loss media por token (cross-entropy)\n",
    "        ppl = float(np.exp(loss))\n",
    "        perplexities.append(ppl)\n",
    "\n",
    "# print(\"Perplexities (primeros 10):\", perplexities[:10])\n",
    "print(\"Mean perplexity:\", float(np.mean(perplexities)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f984287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c7323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:15<12:27,  3.81s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 109.75 MiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 367.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     30\u001b[39m referencias = [\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMi amigo tiene esquizofrenia y alucinaciones.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEl paciente habla con ideas delirantes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m ]\n\u001b[32m     34\u001b[39m generados = [\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMi amigo con esquizofrenia habla de escuchar voces.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEl paciente expresa pensamientos desconectados.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m sim_scores = \u001b[43mbatch_semantic_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSimilitudes semánticas por texto:\u001b[39m\u001b[33m'\u001b[39m, sim_scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mbatch_semantic_similarity\u001b[39m\u001b[34m(reference_texts, generated_texts)\u001b[39m\n\u001b[32m     19\u001b[39m similarities = []\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m,\u001b[32m200\u001b[39m,\u001b[32m1\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     ref_emb = \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     gen_emb = embed_texts(generated_texts[i:i+\u001b[32m4\u001b[39m])\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# convertir a numpy antes de usar sklearn\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36membed_texts\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      4\u001b[39m encoded_input = tokenizer(texts, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m).to(model.device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Activar salida de todos los hidden states para obtener embeddings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     hidden_states = outputs.hidden_states  \u001b[38;5;66;03m# tuple: layers hidden states\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Usar la última capa oculta\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:473\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    472\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFG/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 366.00 MiB. GPU 0 has a total capacity of 3.94 GiB of which 109.75 MiB is free. Including non-PyTorch memory, this process has 3.50 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 367.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed_texts(texts):\n",
    "    encoded_input = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        # Activar salida de todos los hidden states para obtener embeddings\n",
    "        outputs = model(**encoded_input, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # tuple: layers hidden states\n",
    "        # Usar la última capa oculta\n",
    "        last_hidden = hidden_states[-1]\n",
    "        # Promediar embeddings de tokens por cada secuencia (dim=1)\n",
    "        embeddings = last_hidden.mean(dim=1)\n",
    "        # Convertir a float32 y mover a CPU para compatibilidad con sklearn/numpy\n",
    "        embeddings = embeddings.to(torch.float32).cpu()\n",
    "    return embeddings\n",
    "\n",
    "def batch_semantic_similarity(reference_texts, generated_texts):\n",
    "\n",
    "    similarities = []\n",
    "    for i in tqdm(range(0,200,1)):\n",
    "        ref_emb = embed_texts(reference_texts[i:i+4])\n",
    "        gen_emb = embed_texts(generated_texts[i:i+4])\n",
    "        # convertir a numpy antes de usar sklearn\n",
    "        ref_np = [j.unsqueeze(0).numpy() for j in ref_emb]\n",
    "        gen_np = [j.unsqueeze(0).numpy() for j in gen_emb]\n",
    "        sim = [float(cosine_similarity(r, g)[0][0]) for r, g in zip(ref_np, gen_np)]\n",
    "        similarities.extend(sim)\n",
    "    return similarities\n",
    "# Ejemplo\n",
    "referencias = [\n",
    "    \"Mi amigo tiene esquizofrenia y alucinaciones.\",\n",
    "    \"El paciente habla con ideas delirantes.\"\n",
    "]\n",
    "generados = [\n",
    "    \"Mi amigo con esquizofrenia habla de escuchar voces.\",\n",
    "    \"El paciente expresa pensamientos desconectados.\"\n",
    "]\n",
    "\n",
    "sim_scores = batch_semantic_similarity(reales, predictions)\n",
    "print('Similitudes semánticas por texto:', sim_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9c3fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0783429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             attn_implementation='eager',\n",
    "                                             device_map=\"cuda\",dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea7e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 884/884 [13:13<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "reales_only_response = []\n",
    "for real in tqdm(reales):\n",
    "    real_no_response = real[:real.find('### Expected Response:') + len(\"### Expected Response:\")].strip()\n",
    "    inputs = tokenizer(real_no_response, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=None,\n",
    "            do_sample=True,\n",
    "            # temperature=0.2,\n",
    "            # top_p=0.90,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(f\"{output=}\")\n",
    "    # print(f\"{real=}\")\n",
    "    predictions.append(output)\n",
    "    reales_only_response.append(real[real.find('### Expected Response:') + len('### Expected Response:'):].strip())\n",
    "    # predictions.append(output[output.find('### Expected Response:')+len('### Expected Response:'):].strip())\n",
    "    # print(f\"{reales_only_response[-1]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61994add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.00697797889791655\n",
      "ROUGE: {'rouge1': np.float64(0.09016682546380125), 'rouge2': np.float64(0.008694714171673713), 'rougeL': np.float64(0.06030742918384896), 'rougeLsum': np.float64(0.08293171662688542)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando perplexities: 100%|██████████| 884/884 [04:04<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities (primeros 10): [27.466263752307377, 24.612488690215503, 20.933212515663044, 19.543498237411534, 14.093393997144519, 19.28727354375807, 14.007683478939311, 17.30278154319744, 43.48730444818187, 15.256172404578898]\n",
      "Mean perplexity: 22.084054707809567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Para BLEU la librería espera referencias como lista de listas (cada pred puede tener varias refs)\n",
    "references_for_bleu = [[r] for r in reales_only_response]   # [[\"respuesta real 1\"], [\"respuesta real 2\"]]\n",
    "\n",
    "# Opción 1: pasar cadenas y dejar que evaluate haga el tokenizado internamente\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references_for_bleu)\n",
    "print(\"BLEU:\", bleu_result[\"bleu\"])\n",
    "\n",
    "# ROUGE normalmente recibe listas de strings\n",
    "rouge_result = rouge.compute(predictions=predictions, references=reales_only_response)\n",
    "print(\"ROUGE:\", rouge_result)\n",
    "\n",
    "# Calcular perplexity usando el módulo ya cargado\n",
    "perplexities = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(predictions, desc=\"Calculando perplexities\"):\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        # Para modelos causales: pasar labels iguales a input_ids calcula la loss por token\n",
    "        outputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "        loss = outputs.loss.item()  # loss media por token (cross-entropy)\n",
    "        ppl = float(np.exp(loss))\n",
    "        perplexities.append(ppl)\n",
    "\n",
    "print(\"Perplexities (primeros 10):\", perplexities[:10])\n",
    "print(\"Mean perplexity:\", float(np.mean(perplexities)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
